## Import data with Pandas
import time
import scipy as sp
import pandas as pd
import matplotlib.pyplot as plt

def toWindow(data):
    '''
    Asks user if the data needs to be windowed and what the window size
    should be.
    '''

    ncols = data.shape[1] - 1
    fig = plt.figure()
    for i in range(ncols):
        plt.subplot(ncols, 1, i+1)
        plt.plot (data.iloc[:,0], data.iloc[:,i+1])
    plt.show(block=False)

    print('\n')
    win = input('Does the data need to be windowed? [Yes / No] ')
    print('\n')

    if win.upper() == 'YES':
        winmin = int(input('What is the first data point? '))
        print('\n')
        winmax = int(input('What is the last data point? '))

        data = data[(data.Time >= winmin) & (data.Time < winmax)]
        data.reset_index(drop=True, inplace=True)

    plt.close(fig)

    return(data)

def isHeader(path):
    '''
    Determine if a datafile has a header by attempting to convert the first
    row into a float.  Failing a conversion to a float indicates a string which
    should only occur if there is a header such as 'Time;. If the first row
    fails the float conversion, this program returns True.

    This program will not catch headers that can be converted to floats.
    '''

    try:
        headtest = pd.read_csv(path, nrows = 2, header = None, dtype = float)
        return False
    except ValueError:
        return True

def csv2data(path):
    '''
    Given a path to a .csv file, output the information in a data frame
    of the form [Time, R1, R2, ... , Rn], where n is the number of response
    columns in the file.

    If a file is already pre-labeled, maintain those labels.  If the time
    variable is in miliseconds, convert to seconds.
    '''

    ncols = sp.shape(pd.read_csv(path, nrows = 2, header = None))[1]

    ## Determine if file has headers
    headtest = isHeader(path)

    ## Add column names if they are missing
    #  Ensure 'Time' is the first column name
    #  Allows user to label response columns as they want
    if not headtest:
        headers = ['Time']

        for i in range(ncols-1):
            headers.append(('R%i' %(i+1)))

        data = pd.read_csv(path, header = None, dtype='float16')
        data.columns = [headers]
    else:
        data = pd.read_csv(path, dtype='float16')
        data.rename(columns = {data.columns[0]:'Time'}, inplace = True)

    ## Delete Empty columns
    #  * A problem I found importing data from csv files
    #    generated by a Shocklog 298.

    for i in range(ncols-1):
        if pd.isnull(data.iloc[0,i+1]):
            data.drop(data.columns[i+1], axis=1, inplace=True)

    ## Standardize the time column
    #  Time may be input in either seconds or miliseconds.
    #  We need to allow for this and convert ms to s
    #  !!! Assumes the sampling frequency is greater then 10
    #  and less then 10,000 !!!

    dt = data.Time[2] - data.Time[1]
    if dt > 0.1:
        data.Time = data.Time/1000

    ## Remove gravity bias
    for i in range(data.shape[1]-1):
        data.iloc[:,i+1] = data.iloc[:,i+1] - sp.mean(sp.nan_to_num(data.iloc[:,i+1]))

    return(data)

def path2data(path, eventtime = 60):
    '''
    Given the path to either a folder or file name, generate a dataframe
    with all data stacked and filtered.

    Returns a pandas Panel of dataframes of the form:

    [[E0, E1, ..., El], [t0, t1, ... tm], ['Time', R0, R1, ..., Rn]]

    where l is the numbe of events, m is number of data points collected,
    and n is the number of Responses recorded.

    Assumes:
    * All datafiles in the folders have the same number of responses
    recorded. Usually either one or three (X, Y, and Z).
    * All datafiles have the same time step.
    '''
    tic = time.clock()
    pd.options.mode.chained_assignment = None  # default='warn'

    ## Return the results for a single file
    if path[-4:] == '.csv':

        print("\n")
        print("Pulling data from path... \n")
        data_single = csv2data(path)

        print("\n")
        print("Printing overview... \n")
        data_single = toWindow(data_single)

        ncols = data_single.shape[1]
        maxtime = max(data_single.Time)
        eventTime = 60 # [seconds] How long should each event be
        sampPerE = int(sp.floor(data_single.shape[0]/(maxtime/eventTime)))
        numE = int(sp.floor(maxtime/eventTime))
        dt = data_single.Time[2]-data_single.Time[1]
        time_col = data_single.Time[0:numE-1] # Use this column for every event

        print("\n")
        print("Cleaning data... \n")
        data = {}
        for i in range(numE):
            name = 'E' + str(i)
            data[name] = data_single.iloc[(i)*sampPerE:((i+1)*sampPerE-1), :]
            data[name].index = range(len(data[name].index)) #resets the index at 0

            # Set the time to be the same across all events
            data[name].Time = time_col

        data_panel = pd.Panel(data)

        toc = time.clock()

        print('# Samples: ', len(data))
        print('Time to collect and filter data: %.1f [s]' %(toc-tic))

    else:
        ## Format and return the results for a folder

        # Create a list of the files in the folder
        files = glob.glob1(path, '*.csv')

        # Find the average time between recorded points
        timedata = csv2data(path + '/' + files[0])

        dtav = sp.zeros(len(timedata)-1)
        for i in range(len(timedata)-1):
            dtav[i] = (timedata.Time[i+1]-timedata.Time[i])
        dt = sp.mean(dtav)

        ## All of the samples need to be the same length to make averaging
        #  the responses easy.  One method is to pad the shorter samples
        #  with zeros.  This however 'smears' the response.  Another is to
        #  truncate the larger repsonses. This loses some data which may be
        #  important.  Finally, you can increase the complexity of the analyiss
        #  to account for varying sample sizes. For my use case, losing the
        #  data at the end of the response is not critical, and this is the
        #  method I use.
        samp_min = len(timedata)

        ## Pull and label each Event and add them to a pandas Panel
        data = {}
        for i in range(len(files)):
            file_path = path + '/' + files[i]
            name = 'E' + str(i)
            data[name] = csv2data(file_path)

            # Set the time to be the same across all events
            data[name].Time[0] = 0
            for j in range(len(data[name])-1):
                data[name].Time[j+1] = data[name].Time[j] + dt

            # Determine if this event has the fewest samples
            if len(data[name])<samp_min: samp_min = len(data[name])

        data_panel = pd.Panel(data)

        # Set all events to the same sample length
        data_panel = data[:,0:samp_min-1,:]

        toc = time.clock()

        print('# Events: %i' %len(files))
        print('# Samples per Event: %i' %samp_min)
        print('Time to collect and filter data: %.1f [s]' %(toc-tic))

    return data_panel

#data = path2data(path)

#return(data)
